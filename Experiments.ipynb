{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a35c39-a65e-44d3-a3fb-bd7d972602b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import tiktoken\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cf5311-716e-4ea0-923d-352127f79754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTokenizedGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file_path):\n",
    "        data = torch.load(tokenized_file_path)\n",
    "        self.input_ids = data['input_ids']\n",
    "        self.target_ids = data['target_ids']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "# Function to create a dataloader from a pre-tokenized file\n",
    "def create_dataloader_from_tokenized(tokenized_file_path, batch_size=4, \n",
    "                                    shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = PreTokenizedGPTDataset(tokenized_file_path)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        drop_last=drop_last, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f171edc7-0a25-4496-aa9a-f7d70bf05576",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m tokenized_file_path = \u001b[33m\"\u001b[39m\u001b[33m../03_bonus_pretraining_on_gutenberg/tokenized_data/combined_1.pt\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your actual file path\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 2. Create a dataset instance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m dataset = \u001b[43mPreTokenizedGPTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sequences\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 3. Let's examine a sample from the dataset\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mPreTokenizedGPTDataset.__init__\u001b[39m\u001b[34m(self, tokenized_file_path)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenized_file_path):\n\u001b[32m      9\u001b[39m     data = torch.load(tokenized_file_path)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_ids = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mself\u001b[39m.target_ids = data[\u001b[33m'\u001b[39m\u001b[33mtarget_ids\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Define the PreTokenizedGPTDataset class\n",
    "class PreTokenizedGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file_path):\n",
    "        data = torch.load(tokenized_file_path)\n",
    "        self.input_ids = data['input_ids']\n",
    "        self.target_ids = data['target_ids']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# Function to create a dataloader from a pre-tokenized file\n",
    "def create_dataloader_from_tokenized(tokenized_file_path, batch_size=4, \n",
    "                                    shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = PreTokenizedGPTDataset(tokenized_file_path)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        drop_last=drop_last, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "# Example usage:\n",
    "# 1. Initialize with a specific tokenized file\n",
    "tokenized_file_path = \"../03_bonus_pretraining_on_gutenberg/tokenized_data/combined_1.pt\"  # Replace with your actual file path\n",
    "\n",
    "# 2. Create a dataset instance\n",
    "dataset = PreTokenizedGPTDataset(tokenized_file_path)\n",
    "print(f\"Dataset contains {len(dataset)} sequences\")\n",
    "\n",
    "# 3. Let's examine a sample from the dataset\n",
    "sample_input, sample_target = dataset[0]  # Get the first sample\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "print(f\"Sample target shape: {sample_target.shape}\")\n",
    "\n",
    "# 4. Check the first few tokens of the sample\n",
    "print(\"First 10 tokens of input:\", sample_input[:10].tolist())\n",
    "print(\"First 10 tokens of target:\", sample_target[:10].tolist())\n",
    "\n",
    "# 5. Create a dataloader\n",
    "batch_size = 4\n",
    "dataloader = create_dataloader_from_tokenized(\n",
    "    tokenized_file_path,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"Dataloader contains {len(dataloader)} batches\")\n",
    "\n",
    "# 6. Iterate through a batch to see the structure\n",
    "for batch_idx, (input_batch, target_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"Input batch shape: {input_batch.shape}\")  # Should be [batch_size, sequence_length]\n",
    "    print(f\"Target batch shape: {target_batch.shape}\")\n",
    "    \n",
    "    # Only look at the first batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72dc9277-a984-4f62-8fa7-b64666779592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'torch.Tensor'>\n",
      "Tensor shape: torch.Size([28909072])\n",
      "First 10 elements: tensor([ 220,  220,  220,  220,  220, 4091,  642, 3104, 4310,   12])\n",
      "Dataset contains 28908048 sequences\n",
      "Sample input shape: torch.Size([1024])\n",
      "Sample target shape: torch.Size([1024])\n",
      "First 10 tokens of input: [220, 220, 220, 220, 220, 4091, 642, 3104, 4310, 12]\n",
      "First 10 tokens of target: [220, 220, 220, 220, 4091, 642, 3104, 4310, 12, 71]\n",
      "Dataloader contains 7227012 batches\n",
      "Batch 0:\n",
      "Input batch shape: torch.Size([4, 1024])\n",
      "Target batch shape: torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "# First, let's check the actual structure of your tokenized file\n",
    "import torch\n",
    "\n",
    "# Replace with your actual file path\n",
    "tokenized_file_path = \"../03_bonus_pretraining_on_gutenberg/tokenized_data/combined_1.pt\"\n",
    "\n",
    "# Load the file and inspect its structure\n",
    "data = torch.load(tokenized_file_path)\n",
    "\n",
    "# Check what type of data it is\n",
    "print(f\"Data type: {type(data)}\")\n",
    "\n",
    "# If it's a tensor, check its shape\n",
    "if isinstance(data, torch.Tensor):\n",
    "    print(f\"Tensor shape: {data.shape}\")\n",
    "    print(f\"First 10 elements: {data[:10]}\")\n",
    "    \n",
    "# If it's a dictionary, check its keys\n",
    "elif isinstance(data, dict):\n",
    "    print(f\"Dictionary keys: {list(data.keys())}\")\n",
    "    \n",
    "# Now, let's define a more flexible dataset class that can handle different formats\n",
    "\n",
    "class FlexibleTokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_file_path, context_length=1024, stride=768):\n",
    "        # Load the tokenized data\n",
    "        data = torch.load(tokenized_file_path)\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Handle different possible formats\n",
    "        if isinstance(data, dict) and 'input_ids' in data and 'target_ids' in data:\n",
    "            # The data is already in input/target format\n",
    "            self.input_ids = data['input_ids']\n",
    "            self.target_ids = data['target_ids']\n",
    "            self.preformatted = True\n",
    "        elif isinstance(data, torch.Tensor):\n",
    "            # The data is just a tensor of tokens, so we need to create the sliding windows\n",
    "            self.tokens = data\n",
    "            self.preformatted = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data format: {type(data)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.preformatted:\n",
    "            return len(self.input_ids)\n",
    "        else:\n",
    "            # Number of possible windows\n",
    "            return max(0, len(self.tokens) - self.context_length)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.preformatted:\n",
    "            # Return the pre-formatted input/target pair\n",
    "            return self.input_ids[idx], self.target_ids[idx]\n",
    "        else:\n",
    "            # Create the input/target pair on-the-fly\n",
    "            start_idx = idx\n",
    "            end_idx = start_idx + self.context_length\n",
    "            \n",
    "            input_ids = self.tokens[start_idx:end_idx]\n",
    "            target_ids = self.tokens[start_idx+1:end_idx+1]\n",
    "            \n",
    "            return input_ids, target_ids\n",
    "\n",
    "# Function to create a dataloader with the flexible dataset\n",
    "def create_flexible_dataloader(tokenized_file_path, batch_size=4, \n",
    "                             context_length=1024, stride=768,\n",
    "                             shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = FlexibleTokenizedDataset(\n",
    "        tokenized_file_path,\n",
    "        context_length=context_length,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        drop_last=drop_last, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Let's try the new class\n",
    "try:\n",
    "    dataset = FlexibleTokenizedDataset(tokenized_file_path)\n",
    "    print(f\"Dataset contains {len(dataset)} sequences\")\n",
    "    \n",
    "    # Get a sample\n",
    "    if len(dataset) > 0:\n",
    "        sample_input, sample_target = dataset[0]\n",
    "        print(f\"Sample input shape: {sample_input.shape}\")\n",
    "        print(f\"Sample target shape: {sample_target.shape}\")\n",
    "        print(f\"First 10 tokens of input: {sample_input[:10].tolist()}\")\n",
    "        print(f\"First 10 tokens of target: {sample_target[:10].tolist()}\")\n",
    "    \n",
    "    # Create a dataloader\n",
    "    batch_size = 4\n",
    "    dataloader = create_flexible_dataloader(\n",
    "        tokenized_file_path,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(f\"Dataloader contains {len(dataloader)} batches\")\n",
    "    \n",
    "    # Check a batch\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"Input batch shape: {input_batch.shape}\")\n",
    "        print(f\"Target batch shape: {target_batch.shape}\")\n",
    "        break\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee487d5-532c-4aaa-86e4-8fba5968012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 30268742 sequences\n",
      "Sample input shape: torch.Size([1024])\n",
      "\n",
      "--- Sample input text (first 100 tokens) ---\n",
      "\n",
      "                                LONDON\n",
      "                        FREDERICK WARNE AND CO.\n",
      "                             AND NEW YORK\n",
      "\n",
      "\n",
      "--- Sample target text (first 100 tokens) ---\n",
      "                                LONDON\n",
      "                        FREDERICK WARNE AND CO.\n",
      "                             AND NEW YORK\n",
      " \n",
      "\n",
      "--- Input/target relationship (first 10 tokens) ---\n",
      "Input tokens: [198, 220, 220, 220, 220, 220, 220, 220, 220, 220]\n",
      "Target tokens: [220, 220, 220, 220, 220, 220, 220, 220, 220, 220]\n",
      "Notice how the target is shifted one position relative to the input (next-token prediction)\n",
      "\n",
      "Dataloader contains 7567185 batches\n",
      "\n",
      "Batch 0:\n",
      "Batch shapes: input torch.Size([4, 1024]), target torch.Size([4, 1024])\n",
      "\n",
      "--- Example 0 from batch ---\n",
      "Beginning of input sequence:\n",
      " chamber; but their tenure of\n",
      "life is independent of that of their objects, since thought may be\n",
      "prophetic or reminiscent and is intermittent even when its object enjoys\n",
      "a continuous existence. Mental facts are similar to their objects, since\n",
      "things and\n",
      "\n",
      "Beginning of target sequence:\n",
      "; but their tenure of\n",
      "life is independent of that of their objects, since thought may be\n",
      "prophetic or reminiscent and is intermittent even when its object enjoys\n",
      "a continuous existence. Mental facts are similar to their objects, since\n",
      "things and images\n",
      "\n",
      "--- Most common tokens ---\n",
      "Token 220: ' ' (count: 1104)\n",
      "Token 198: '\n",
      "' (count: 686)\n",
      "Token 11: ',' (count: 486)\n",
      "Token 262: ' the' (count: 390)\n",
      "Token 286: ' of' (count: 330)\n",
      "Token 290: ' and' (count: 230)\n",
      "Token 284: ' to' (count: 210)\n",
      "Token 13: '.' (count: 180)\n",
      "Token 287: ' in' (count: 160)\n",
      "Token 389: ' are' (count: 150)\n",
      "Token 326: ' that' (count: 130)\n",
      "Token 257: ' a' (count: 110)\n",
      "Token 355: ' as' (count: 100)\n",
      "Token 26: ';' (count: 90)\n",
      "Token 12: '-' (count: 80)\n",
      "Token 468: ' has' (count: 80)\n",
      "Token 484: ' they' (count: 80)\n",
      "Token 606: ' them' (count: 70)\n",
      "Token 464: 'The' (count: 60)\n",
      "Token 587: ' been' (count: 60)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "# Flexible dataset class with token decoding capability\n",
    "class FlexibleTokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_file_path, context_length=1024, stride=768):\n",
    "        # Load the tokenized data\n",
    "        data = torch.load(tokenized_file_path)\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Initialize tokenizer for decoding\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # Handle different possible formats\n",
    "        if isinstance(data, dict) and 'input_ids' in data and 'target_ids' in data:\n",
    "            # The data is already in input/target format\n",
    "            self.input_ids = data['input_ids']\n",
    "            self.target_ids = data['target_ids']\n",
    "            self.preformatted = True\n",
    "        elif isinstance(data, torch.Tensor):\n",
    "            # The data is just a tensor of tokens, so we need to create the sliding windows\n",
    "            self.tokens = data\n",
    "            self.preformatted = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data format: {type(data)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.preformatted:\n",
    "            return len(self.input_ids)\n",
    "        else:\n",
    "            # Number of possible windows\n",
    "            return max(0, len(self.tokens) - self.context_length)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.preformatted:\n",
    "            # Return the pre-formatted input/target pair\n",
    "            return self.input_ids[idx], self.target_ids[idx]\n",
    "        else:\n",
    "            # Create the input/target pair on-the-fly\n",
    "            start_idx = idx\n",
    "            end_idx = start_idx + self.context_length\n",
    "            \n",
    "            input_ids = self.tokens[start_idx:end_idx]\n",
    "            target_ids = self.tokens[start_idx+1:end_idx+1]\n",
    "            \n",
    "            return input_ids, target_ids\n",
    "    \n",
    "    def decode_tokens(self, tokens):\n",
    "        \"\"\"Decode tokens back to text\"\"\"\n",
    "        if isinstance(tokens, torch.Tensor):\n",
    "            tokens = tokens.tolist()\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "# Function to create a dataloader with the flexible dataset\n",
    "def create_flexible_dataloader(tokenized_file_path, batch_size=4, \n",
    "                             context_length=1024, stride=768,\n",
    "                             shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = FlexibleTokenizedDataset(\n",
    "        tokenized_file_path,\n",
    "        context_length=context_length,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        drop_last=drop_last, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader, dataset\n",
    "\n",
    "# Demonstration of using the decoder\n",
    "# Replace with your actual file path\n",
    "tokenized_file_path = \"../03_bonus_pretraining_on_gutenberg/tokenized_data/combined_4.pt\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = FlexibleTokenizedDataset(tokenized_file_path)\n",
    "print(f\"Dataset contains {len(dataset)} sequences\")\n",
    "\n",
    "# Get a sample\n",
    "if len(dataset) > 0:\n",
    "    sample_input, sample_target = dataset[0]\n",
    "    print(f\"Sample input shape: {sample_input.shape}\")\n",
    "    \n",
    "    # Decode a portion of the input tokens\n",
    "    print(\"\\n--- Sample input text (first 100 tokens) ---\")\n",
    "    decoded_input = dataset.decode_tokens(sample_input[:100])\n",
    "    print(decoded_input)\n",
    "    \n",
    "    print(\"\\n--- Sample target text (first 100 tokens) ---\")\n",
    "    decoded_target = dataset.decode_tokens(sample_target[:100])\n",
    "    print(decoded_target)\n",
    "    \n",
    "    # Show the relationship between input and target\n",
    "    print(\"\\n--- Input/target relationship (first 10 tokens) ---\")\n",
    "    print(f\"Input tokens: {sample_input[:10].tolist()}\")\n",
    "    print(f\"Target tokens: {sample_target[:10].tolist()}\")\n",
    "    print(\"Notice how the target is shifted one position relative to the input (next-token prediction)\")\n",
    "\n",
    "# Create a dataloader\n",
    "batch_size = 4\n",
    "dataloader, _ = create_flexible_dataloader(\n",
    "    tokenized_file_path,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"\\nDataloader contains {len(dataloader)} batches\")\n",
    "\n",
    "# Function to examine a batch with decoding\n",
    "def examine_batch(batch, dataset):\n",
    "    input_batch, target_batch = batch\n",
    "    print(f\"Batch shapes: input {input_batch.shape}, target {target_batch.shape}\")\n",
    "    \n",
    "    # Decode the first example in the batch\n",
    "    example_idx = 0\n",
    "    print(f\"\\n--- Example {example_idx} from batch ---\")\n",
    "    input_example = input_batch[example_idx]\n",
    "    target_example = target_batch[example_idx]\n",
    "    \n",
    "    # Decode the first 50 tokens\n",
    "    print(\"Beginning of input sequence:\")\n",
    "    print(dataset.decode_tokens(input_example[:50]))\n",
    "    \n",
    "    print(\"\\nBeginning of target sequence:\")\n",
    "    print(dataset.decode_tokens(target_example[:50]))\n",
    "\n",
    "# Check a batch\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {batch_idx}:\")\n",
    "    examine_batch(batch, dataset)\n",
    "    break  # Just examine the first batch\n",
    "\n",
    "# Utility function to analyze token distribution\n",
    "def analyze_token_distribution(dataset, num_samples=10):\n",
    "    \"\"\"Analyze the distribution of tokens in a dataset\"\"\"\n",
    "    token_counts = {}\n",
    "    \n",
    "    # Sample from the dataset\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        idx = i  # You could make this random: random.randint(0, len(dataset)-1)\n",
    "        input_ids, _ = dataset[idx]\n",
    "        \n",
    "        # Count tokens\n",
    "        for token in input_ids.tolist():\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "    \n",
    "    # Get most common tokens\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    most_common = sorted_tokens[:20]  # Top 20 most common tokens\n",
    "    \n",
    "    print(\"\\n--- Most common tokens ---\")\n",
    "    for token, count in most_common:\n",
    "        decoded = dataset.decode_tokens([token])\n",
    "        print(f\"Token {token}: '{decoded}' (count: {count})\")\n",
    "\n",
    "# Analyze token distribution\n",
    "analyze_token_distribution(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a2b6f-f3f9-4430-af01-ab96b371b8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
